{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TlYSYgarG97"
      },
      "outputs": [],
      "source": [
        "# NLP DATA CLEANING AND PREPROCESSING:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import spacy"
      ],
      "metadata": {
        "id": "sK0GnKWHrb9G"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "He is a boy.\n",
        "he,is, a =75%\n",
        "boy = 25%"
      ],
      "metadata": {
        "id": "hvxPMotHslWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Stop words are common words that are removed from text before processing in natural language processing (NLP).\n",
        "This helps to improve the efficiency and accuracy of NLP tasks.\n",
        "Examples of stop words\n",
        "\"The, \"And, \"Is, \"A, \"An, \"In, \"On, \"At, \"With, and \"He.\n",
        "\n",
        "Why remove stop words?\n",
        "Reduce noise: Stop words can reduce the amount of noise in text data.\n",
        "Improve efficiency: Stop words can improve the efficiency of text-based algorithms.\n",
        "Save memory: Stop words can help to save memory.\n",
        "Improve performance: Stop words can improve the performance of machine learning models."
      ],
      "metadata": {
        "id": "B0wSfegstMhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ],
      "metadata": {
        "id": "_TZjCqHft3Sr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2nmW5A0t-f7",
        "outputId": "87c0d861-83bb-4428-af00-16aebce02b11"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CiaITTvtiM0",
        "outputId": "e5d2299c-f12a-419c-8241-fd74d54c3d93"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words1 = set(stopwords.words('french'))\n",
        "stop_words1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSydQN8xv3aU",
        "outputId": "6a1113ad-0114-4b5a-9053-a95bbea1f2f5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ai',\n",
              " 'aie',\n",
              " 'aient',\n",
              " 'aies',\n",
              " 'ait',\n",
              " 'as',\n",
              " 'au',\n",
              " 'aura',\n",
              " 'aurai',\n",
              " 'auraient',\n",
              " 'aurais',\n",
              " 'aurait',\n",
              " 'auras',\n",
              " 'aurez',\n",
              " 'auriez',\n",
              " 'aurions',\n",
              " 'aurons',\n",
              " 'auront',\n",
              " 'aux',\n",
              " 'avaient',\n",
              " 'avais',\n",
              " 'avait',\n",
              " 'avec',\n",
              " 'avez',\n",
              " 'aviez',\n",
              " 'avions',\n",
              " 'avons',\n",
              " 'ayant',\n",
              " 'ayante',\n",
              " 'ayantes',\n",
              " 'ayants',\n",
              " 'ayez',\n",
              " 'ayons',\n",
              " 'c',\n",
              " 'ce',\n",
              " 'ces',\n",
              " 'd',\n",
              " 'dans',\n",
              " 'de',\n",
              " 'des',\n",
              " 'du',\n",
              " 'elle',\n",
              " 'en',\n",
              " 'es',\n",
              " 'est',\n",
              " 'et',\n",
              " 'eu',\n",
              " 'eue',\n",
              " 'eues',\n",
              " 'eurent',\n",
              " 'eus',\n",
              " 'eusse',\n",
              " 'eussent',\n",
              " 'eusses',\n",
              " 'eussiez',\n",
              " 'eussions',\n",
              " 'eut',\n",
              " 'eux',\n",
              " 'eûmes',\n",
              " 'eût',\n",
              " 'eûtes',\n",
              " 'furent',\n",
              " 'fus',\n",
              " 'fusse',\n",
              " 'fussent',\n",
              " 'fusses',\n",
              " 'fussiez',\n",
              " 'fussions',\n",
              " 'fut',\n",
              " 'fûmes',\n",
              " 'fût',\n",
              " 'fûtes',\n",
              " 'il',\n",
              " 'ils',\n",
              " 'j',\n",
              " 'je',\n",
              " 'l',\n",
              " 'la',\n",
              " 'le',\n",
              " 'les',\n",
              " 'leur',\n",
              " 'lui',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'mais',\n",
              " 'me',\n",
              " 'mes',\n",
              " 'moi',\n",
              " 'mon',\n",
              " 'même',\n",
              " 'n',\n",
              " 'ne',\n",
              " 'nos',\n",
              " 'notre',\n",
              " 'nous',\n",
              " 'on',\n",
              " 'ont',\n",
              " 'ou',\n",
              " 'par',\n",
              " 'pas',\n",
              " 'pour',\n",
              " 'qu',\n",
              " 'que',\n",
              " 'qui',\n",
              " 's',\n",
              " 'sa',\n",
              " 'se',\n",
              " 'sera',\n",
              " 'serai',\n",
              " 'seraient',\n",
              " 'serais',\n",
              " 'serait',\n",
              " 'seras',\n",
              " 'serez',\n",
              " 'seriez',\n",
              " 'serions',\n",
              " 'serons',\n",
              " 'seront',\n",
              " 'ses',\n",
              " 'soient',\n",
              " 'sois',\n",
              " 'soit',\n",
              " 'sommes',\n",
              " 'son',\n",
              " 'sont',\n",
              " 'soyez',\n",
              " 'soyons',\n",
              " 'suis',\n",
              " 'sur',\n",
              " 't',\n",
              " 'ta',\n",
              " 'te',\n",
              " 'tes',\n",
              " 'toi',\n",
              " 'ton',\n",
              " 'tu',\n",
              " 'un',\n",
              " 'une',\n",
              " 'vos',\n",
              " 'votre',\n",
              " 'vous',\n",
              " 'y',\n",
              " 'à',\n",
              " 'étaient',\n",
              " 'étais',\n",
              " 'était',\n",
              " 'étant',\n",
              " 'étante',\n",
              " 'étantes',\n",
              " 'étants',\n",
              " 'étiez',\n",
              " 'étions',\n",
              " 'été',\n",
              " 'étée',\n",
              " 'étées',\n",
              " 'étés',\n",
              " 'êtes'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words1 = set(stopwords.words('german'))\n",
        "stop_words1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as7s8X0NwP5j",
        "outputId": "0143e7bf-1714-491e-ddf6-c97eed40e192"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'aber',\n",
              " 'alle',\n",
              " 'allem',\n",
              " 'allen',\n",
              " 'aller',\n",
              " 'alles',\n",
              " 'als',\n",
              " 'also',\n",
              " 'am',\n",
              " 'an',\n",
              " 'ander',\n",
              " 'andere',\n",
              " 'anderem',\n",
              " 'anderen',\n",
              " 'anderer',\n",
              " 'anderes',\n",
              " 'anderm',\n",
              " 'andern',\n",
              " 'anderr',\n",
              " 'anders',\n",
              " 'auch',\n",
              " 'auf',\n",
              " 'aus',\n",
              " 'bei',\n",
              " 'bin',\n",
              " 'bis',\n",
              " 'bist',\n",
              " 'da',\n",
              " 'damit',\n",
              " 'dann',\n",
              " 'das',\n",
              " 'dass',\n",
              " 'dasselbe',\n",
              " 'dazu',\n",
              " 'daß',\n",
              " 'dein',\n",
              " 'deine',\n",
              " 'deinem',\n",
              " 'deinen',\n",
              " 'deiner',\n",
              " 'deines',\n",
              " 'dem',\n",
              " 'demselben',\n",
              " 'den',\n",
              " 'denn',\n",
              " 'denselben',\n",
              " 'der',\n",
              " 'derer',\n",
              " 'derselbe',\n",
              " 'derselben',\n",
              " 'des',\n",
              " 'desselben',\n",
              " 'dessen',\n",
              " 'dich',\n",
              " 'die',\n",
              " 'dies',\n",
              " 'diese',\n",
              " 'dieselbe',\n",
              " 'dieselben',\n",
              " 'diesem',\n",
              " 'diesen',\n",
              " 'dieser',\n",
              " 'dieses',\n",
              " 'dir',\n",
              " 'doch',\n",
              " 'dort',\n",
              " 'du',\n",
              " 'durch',\n",
              " 'ein',\n",
              " 'eine',\n",
              " 'einem',\n",
              " 'einen',\n",
              " 'einer',\n",
              " 'eines',\n",
              " 'einig',\n",
              " 'einige',\n",
              " 'einigem',\n",
              " 'einigen',\n",
              " 'einiger',\n",
              " 'einiges',\n",
              " 'einmal',\n",
              " 'er',\n",
              " 'es',\n",
              " 'etwas',\n",
              " 'euch',\n",
              " 'euer',\n",
              " 'eure',\n",
              " 'eurem',\n",
              " 'euren',\n",
              " 'eurer',\n",
              " 'eures',\n",
              " 'für',\n",
              " 'gegen',\n",
              " 'gewesen',\n",
              " 'hab',\n",
              " 'habe',\n",
              " 'haben',\n",
              " 'hat',\n",
              " 'hatte',\n",
              " 'hatten',\n",
              " 'hier',\n",
              " 'hin',\n",
              " 'hinter',\n",
              " 'ich',\n",
              " 'ihm',\n",
              " 'ihn',\n",
              " 'ihnen',\n",
              " 'ihr',\n",
              " 'ihre',\n",
              " 'ihrem',\n",
              " 'ihren',\n",
              " 'ihrer',\n",
              " 'ihres',\n",
              " 'im',\n",
              " 'in',\n",
              " 'indem',\n",
              " 'ins',\n",
              " 'ist',\n",
              " 'jede',\n",
              " 'jedem',\n",
              " 'jeden',\n",
              " 'jeder',\n",
              " 'jedes',\n",
              " 'jene',\n",
              " 'jenem',\n",
              " 'jenen',\n",
              " 'jener',\n",
              " 'jenes',\n",
              " 'jetzt',\n",
              " 'kann',\n",
              " 'kein',\n",
              " 'keine',\n",
              " 'keinem',\n",
              " 'keinen',\n",
              " 'keiner',\n",
              " 'keines',\n",
              " 'können',\n",
              " 'könnte',\n",
              " 'machen',\n",
              " 'man',\n",
              " 'manche',\n",
              " 'manchem',\n",
              " 'manchen',\n",
              " 'mancher',\n",
              " 'manches',\n",
              " 'mein',\n",
              " 'meine',\n",
              " 'meinem',\n",
              " 'meinen',\n",
              " 'meiner',\n",
              " 'meines',\n",
              " 'mich',\n",
              " 'mir',\n",
              " 'mit',\n",
              " 'muss',\n",
              " 'musste',\n",
              " 'nach',\n",
              " 'nicht',\n",
              " 'nichts',\n",
              " 'noch',\n",
              " 'nun',\n",
              " 'nur',\n",
              " 'ob',\n",
              " 'oder',\n",
              " 'ohne',\n",
              " 'sehr',\n",
              " 'sein',\n",
              " 'seine',\n",
              " 'seinem',\n",
              " 'seinen',\n",
              " 'seiner',\n",
              " 'seines',\n",
              " 'selbst',\n",
              " 'sich',\n",
              " 'sie',\n",
              " 'sind',\n",
              " 'so',\n",
              " 'solche',\n",
              " 'solchem',\n",
              " 'solchen',\n",
              " 'solcher',\n",
              " 'solches',\n",
              " 'soll',\n",
              " 'sollte',\n",
              " 'sondern',\n",
              " 'sonst',\n",
              " 'um',\n",
              " 'und',\n",
              " 'uns',\n",
              " 'unser',\n",
              " 'unsere',\n",
              " 'unserem',\n",
              " 'unseren',\n",
              " 'unseres',\n",
              " 'unter',\n",
              " 'viel',\n",
              " 'vom',\n",
              " 'von',\n",
              " 'vor',\n",
              " 'war',\n",
              " 'waren',\n",
              " 'warst',\n",
              " 'was',\n",
              " 'weg',\n",
              " 'weil',\n",
              " 'weiter',\n",
              " 'welche',\n",
              " 'welchem',\n",
              " 'welchen',\n",
              " 'welcher',\n",
              " 'welches',\n",
              " 'wenn',\n",
              " 'werde',\n",
              " 'werden',\n",
              " 'wie',\n",
              " 'wieder',\n",
              " 'will',\n",
              " 'wir',\n",
              " 'wird',\n",
              " 'wirst',\n",
              " 'wo',\n",
              " 'wollen',\n",
              " 'wollte',\n",
              " 'während',\n",
              " 'würde',\n",
              " 'würden',\n",
              " 'zu',\n",
              " 'zum',\n",
              " 'zur',\n",
              " 'zwar',\n",
              " 'zwischen',\n",
              " 'über'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1=\"\"\"India, officially the Republic of India,[j][21] is a country in South Asia. It is the seventh-largest country by area; the most populous country from June 2023 onwards;[22][23] and since its independence in 1947, the world's most populous democracy.[24][25][26] Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west;[k] China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is near Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand, Myanmar, and Indonesia.\"\"\"\n",
        "text1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "Wt7uV-K_xCGb",
        "outputId": "7b5983d9-11e8-4f49-8527-e1843a471eef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"India, officially the Republic of India,[j][21] is a country in South Asia. It is the seventh-largest country by area; the most populous country from June 2023 onwards;[22][23] and since its independence in 1947, the world's most populous democracy.[24][25][26] Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west;[k] China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is near Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand, Myanmar, and Indonesia.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower case convert:\n",
        "\n",
        "lower_text=text1.lower()\n",
        "lower_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "JbmB5Pjrxilk",
        "outputId": "76c4f5ef-64ec-4f0f-f8ac-1a1a3a390fc7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"india, officially the republic of india,[j][21] is a country in south asia. it is the seventh-largest country by area; the most populous country from june 2023 onwards;[22][23] and since its independence in 1947, the world's most populous democracy.[24][25][26] bounded by the indian ocean on the south, the arabian sea on the southwest, and the bay of bengal on the southeast, it shares land borders with pakistan to the west;[k] china, nepal, and bhutan to the north; and bangladesh and myanmar to the east. in the indian ocean, india is near sri lanka and the maldives; its andaman and nicobar islands share a maritime border with thailand, myanmar, and indonesia.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming and  Lemmatization"
      ],
      "metadata": {
        "id": "RjOC4GPfyNaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Stemming is a natural language processing (NLP)\n",
        "technique that reduces words to their root form.\n",
        "It helps NLP algorithms understand the meaning of words\n",
        "and process language more efficiently.\n",
        "\n",
        "How does stemming work?\n",
        "Stemming removes suffixes from words to get to their root form.\n",
        " For example, \"running\", \"runs\", and \"runner\" all stem to \"run\".\n",
        "Stemming helps search engines and information retrieval systems\n",
        "find relevant results.\n",
        "For example, if a user searches for \"meditate\", they may get results\n",
        "for \"meditates\" and \"meditation\".\n",
        "\n",
        "Benefits of stemming\n",
        "Stemming helps reduce the dimensionality of text data.\n",
        "Stemming improves the efficiency of text processing algorithms.\n",
        "Stemming helps improve search performance"
      ],
      "metadata": {
        "id": "L9XSSkQCyjTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stemminng : word root form : suffixes or prefixes\n",
        "runner : run\n",
        "studies : studi\n",
        "happiness : happi"
      ],
      "metadata": {
        "id": "mog8XSZR1lyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Lemmatization is a text processing technique in natural language processing (NLP)\n",
        "that reduces words to their root form.\n",
        " It's used to group together inflected forms of a word so they can be analyzed\n",
        "  as a single item.\n",
        "\n",
        "How it works\n",
        "Analyzes the word's context, including its use in the sentence and the larger text\n",
        "Determines the word's intended part of speech\n",
        "Compares the word to a dictionary to find its lemma, or dictionary form\n",
        "Groups together different inflected forms of the word under the lemma\n",
        "Examples\n",
        "The verb \"running\" would be lemmatized as \"run\"\n",
        "The word \"better\" would be lemmatized as \"good\"\n",
        "Benefits\n",
        "Lemmatization makes tools like chatbots and search engine queries more effective\n",
        " and accurate by linking words with similar meanings.\n",
        "  It also helps to provide a more accurate language representation\n",
        "  by handling different grammatical categories and tenses.\n",
        "Lemmatization is different from stemming, which simply chops off the end of a word"
      ],
      "metadata": {
        "id": "5ZZxXbI50gfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemma : dictionary form\n",
        "\n",
        "runner : run\n",
        "studies : study\n",
        "happiness : happy"
      ],
      "metadata": {
        "id": "ZdC0lxcZ2Qhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "# from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ],
      "metadata": {
        "id": "lFvznhWQ3AHi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"NLP class is amazing! Can't wait to learn more at www.example.com. the contact number is 88888\"\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5CuYWIQm3YFT",
        "outputId": "cf604bb0-acae-450b-fb14-a6ef659c5853"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"NLP class is amazing! Can't wait to learn more at www.example.com. the contact number is 88888\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jP6Ec7zW4MKK",
        "outputId": "2621923b-5e80-4238-8dcc-5080b2e7953d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens1 = word_tokenize(text)\n",
        "word_tokens1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiM_eX704HDx",
        "outputId": "7c26e733-d3b9-4d08-c17c-8b57472cc3c1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP',\n",
              " 'class',\n",
              " 'is',\n",
              " 'amazing',\n",
              " '!',\n",
              " 'Ca',\n",
              " \"n't\",\n",
              " 'wait',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'more',\n",
              " 'at',\n",
              " 'www.example.com',\n",
              " '.',\n",
              " 'the',\n",
              " 'contact',\n",
              " 'number',\n",
              " 'is',\n",
              " '88888']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "PJ6FTq6B3p5B"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_tokens = [stemmer.stem(word) for word in word_tokens1]\n",
        "\n",
        "stemmed_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ6nd6ni4eIi",
        "outputId": "25c3da73-f836-42f0-b7c5-a5cb8e93a77c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nlp',\n",
              " 'class',\n",
              " 'is',\n",
              " 'amaz',\n",
              " '!',\n",
              " 'ca',\n",
              " \"n't\",\n",
              " 'wait',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'more',\n",
              " 'at',\n",
              " 'www.example.com',\n",
              " '.',\n",
              " 'the',\n",
              " 'contact',\n",
              " 'number',\n",
              " 'is',\n",
              " '88888']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words=['running','happiness', 'run,studies', 'study', 'cars', 'care', 'careful']"
      ],
      "metadata": {
        "id": "hVgGMWsl5YvV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in words]\n",
        "stemmed_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSZKvHiS5k8Z",
        "outputId": "e8abe26a-f2d7-4dce-a578-7562e7b08863"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['run', 'happi', 'run,studi', 'studi', 'car', 'care', 'care']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n"
      ],
      "metadata": {
        "id": "mQ8pE1_n6Eyh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zsFtLap6dPC",
        "outputId": "db664ff7-69f5-436d-d18e-18ced72a4e8c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "lemmatized_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tcbdB446Lz6",
        "outputId": "1d42c48e-55c5-4455-c63d-6c12d1df3fca"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['running', 'happiness', 'run,studies', 'study', 'car', 'care', 'careful']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# actual lists :words=['running','happiness', 'run,studies', 'study', 'cars', 'care', 'careful']\n",
        "#stemming :           ['run', 'happi', 'run,studi', 'studi', 'car', 'care', 'care']\n",
        "# Lemmatization :     ['running', 'happiness', 'run,studies', 'study', 'car', 'care', 'careful']"
      ],
      "metadata": {
        "id": "tmXw4T8s6vdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Application\t            Stemming\t            Lemmatization\n",
        "Search Engines\t     ✅ (Fast, approximate)\t ❌ (Too slow)\n",
        "Chatbots\t           ❌ (Incorrect words)\t   ✅ (More accuracy)\n",
        "Sentiment Analysis\t ❌ (Errors in meaning)\t ✅ (Better accuracy)\n",
        "Spam Detection\t     ✅ (Good for speed)\t     ❌ (Not needed)\n",
        "Machine Translation\t ❌ (Not context-aware)\t ✅ (Better results)"
      ],
      "metadata": {
        "id": "TUJx3Kva9dRX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}